{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing multielectrode GEMsort algorithm using adaptive threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import apply_along_axis as apply\n",
    "from scipy.signal import fftconvolve\n",
    "from scipy import signal\n",
    "from GEMsort_func import GEMsort\n",
    "from Smoothing_data import Smoothing_filter\n",
    "from Cut_func import cut\n",
    "from scipy.signal import butter, lfilter\n",
    "from copy import deepcopy\n",
    "from numpy.linalg import svd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import struct\n",
    "import time\n",
    "import numpy.matlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading and plotting the data\n",
    "np.random.seed(10)\n",
    "path = '/.../synthetic data' \n",
    "\n",
    "data=np.load(path + '/multi_16ch_usualdata.npy', allow_pickle=True)\n",
    "# data = np.load(path + '/multi_16ch_samecells.npy', allow_pickle=True) #this is the similar cells (for two cells)  dataset\n",
    "# data=np.load(path + '/multi_16ch_sametime.npy', allow_pickle=True) #this is the same firing time (for two cells) dataset\n",
    "\n",
    "X = data\n",
    "\n",
    "# %matplotlib inline\n",
    "# for i in range(16):\n",
    "#     plt.plot(X[i,:][0:10000])\n",
    "# # plt.show()\n",
    "\n",
    "#Smoothing filter\n",
    "#%matplotlib qt\n",
    "\n",
    "for i in range(16):\n",
    "    data_raw = X[i, :]\n",
    "#     plt.plot(data_raw)\n",
    "    filtered_data = Smoothing_filter(data_raw, low=10, high=5000, sf=80000, order=2)\n",
    "#     plt.plot(filtered_data, c='r')\n",
    "#     plt.show()\n",
    "    \n",
    "    X[i, :] = filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot heatmap \n",
    "np.random.seed(0)\n",
    "sns.set_theme()\n",
    "uniform_data = X[:, 0:2000]\n",
    "# ax = sns.heatmap(uniform_data, cmap=\"YlGnBu\", xticklabels=False)#, vmin=0.2, vmax=1) #cmap=\"Greens\",\"BuPu\",\"Blues\"\n",
    "# ax = sns.heatmap(uniform_data, cmap=\"YlGnBu\", xticklabels=False, center=0)#, vmin=0.2, vmax=1)\n",
    "\n",
    "uniform_data = X[:, 0:1000]\n",
    "ax = sns.heatmap(uniform_data, cmap=\"YlGnBu\", xticklabels=False)#, vmin=0.2, vmax=1)\n",
    "# ax = sns.heatmap(uniform_data, cmap=\"YlGnBu\", xticklabels=False, center=0)#, vmin=0.2, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the threshold for real-time spike detection.\n",
    "\n",
    "def initialize_threshold(signal, init_segment_size=5000, K=3):\n",
    "    init_segment = signal[:init_segment_size]\n",
    "    initial_std = np.std(init_segment)\n",
    "    initial_mean = np.mean(init_segment)\n",
    "    initial_threshold = initial_mean + K * initial_std\n",
    "\n",
    "    return initial_threshold\n",
    "\n",
    "# adding dynamic threshold\n",
    "def calculate_dynamic_threshold(signal, window_size, K):\n",
    "    if len(signal) < window_size:\n",
    "        threshold = initialize_threshold\n",
    "    recent_segment = signal[-window_size:]\n",
    "    recent_mean = np.mean(recent_segment)\n",
    "    recent_std = np.std(recent_segment)\n",
    "\n",
    "    threshold = np.mean(recent_segment) + K * recent_std\n",
    "#     print('np.mean(recent_segment)=', np.mean(recent_segment))\n",
    "#     print('threshold=', threshold)\n",
    "    return threshold\n",
    "\n",
    "# Example usage:\n",
    "# Example: data_stream = np.random.normal(0, 1, size=100000)  # Simulated neural data\n",
    "# window_size = 5000\n",
    "# K = 3\n",
    "# current_threshold = calculate_dynamic_threshold(data_stream, window_size, K)\n",
    "\n",
    "\n",
    "# Assuming X is a 2D array where each row represents a channel\n",
    "Tdis = 50  \n",
    "p_all = np.array([], dtype = object).reshape((1,0))\n",
    "cut_ch = []\n",
    "ch_num = 16\n",
    "T1 = 64\n",
    "# num_win = 10\n",
    "win_length = 5000  # Length of each streaming data section\n",
    "\n",
    "num_samples = X.shape[1]  # Total number of samples in the signal\n",
    "\n",
    "for k in range (ch_num): \n",
    "    cut_ch .append([])\n",
    "    \n",
    "for i in range(16):  # for every channel\n",
    "#     x = X[i, :] \n",
    "    p = []\n",
    "    for start in range(0, num_samples, win_length):\n",
    "#         print(start)\n",
    "        end = min(start + win_length, num_samples)  \n",
    "        x_w = X[i, start:end] \n",
    "\n",
    "        # Calculate dynamic thresholds using the new function\n",
    "        Tp = calculate_dynamic_threshold(x_w, window_size=1000, K=3)\n",
    "        # print('Tp=', Tp)\n",
    "    \n",
    "        p_w,_ = scipy.signal.find_peaks(x_w, height = Tp, distance=Tdis)  # Refine peak detection on detected peaks\n",
    "        p_w += start\n",
    "        p.append(list(p_w))\n",
    "    p = np.concatenate(p)\n",
    "    p = np.array(p)\n",
    "#     print('p=', p)\n",
    "    for j in range(np.array(p).shape[0]):\n",
    "#         print(p[j])\n",
    "        cut_ch[i].append(cut(X[i, :],p[j],T1)[0])\n",
    "    \n",
    "    if p == []:\n",
    "        p = [0] \n",
    "    y_arr = np.array([], dtype = np.int32)\n",
    "    y = p\n",
    "    y_arr = np.append(y_arr,y)\n",
    "    #print(y_arr)\n",
    "    p_all = np.append(p_all, 0)\n",
    "#     print(p_all)\n",
    "    p_all[-1] = y_arr.astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "a = np.hstack(p_all) \n",
    "rowp = a[None, :]\n",
    "\n",
    "channels = []\n",
    "times = []\n",
    "\n",
    "for k in range (np.array(rowp).shape[1]): \n",
    "    channels.append([]) \n",
    "    times.append([])\n",
    "        \n",
    "for k in range (np.array(rowp).shape[1]): #for every peak  \n",
    "    for i in range (p_all.shape[0]): #number of channels \n",
    "        for j in range (p_all[i].shape[0]):\n",
    "            if all ([p_all[i][j] != 0 and np.abs(p_all[i][j] - rowp[0,k]) < 2 and X[i,rowp[0,k]] != 0]): \n",
    "#                 print(p_all[i][j])\n",
    "#                 print(rowp[0,k])\n",
    "                channels[k].append(i) #all the channels number with the same peak(k)\n",
    "                times[k].append(j) #time values for that channel\n",
    "                \n",
    "# rowtt = np.unique(rowf)\n",
    "# for i in range(rowtt.shape[0]):\n",
    "#     if np.array(rowtt[i]).shape[0] == 1:\n",
    "#         print(i)\n",
    "#         channels[1,i] = -1\n",
    "        \n",
    "#old_sig = np.copy(signals) \n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "start_time = time.time()\n",
    "signals_after_corr = deepcopy(cut_ch) \n",
    "\n",
    "th_cor = 0.5\n",
    "maxx = np.zeros((1,np.array(rowp).shape[1]))\n",
    "zeross = 0 #this is the total number of peaks that will set to zero\n",
    "#for k in range (np.array(rowp).shape[1]): \n",
    "for k in range(np.array(rowp).shape[1]): #the peak counter    \n",
    "    #print rowp[0][k] \n",
    "    #print(k)\n",
    "\n",
    "    if all ([channels[k] != [] and np.array(channels[k]).shape[0] != 1]) : \n",
    "        #repetitive elements of rows since they have different rowp (peak time)\n",
    "        #print(channels[k])\n",
    "        maxx[0,k] = -999999\n",
    "        #print(k)\n",
    "        for p in range (np.array(channels[k])[:, None].shape[0]) : #considering number of same peaks in every group           \n",
    "            #print p\n",
    "            for t in range (np.array(channels[k])[:, None].shape[0]):\n",
    "#                 print(t)\n",
    "#                 print(p)\n",
    "\n",
    "#below condition checks that channels be near each other (less than 3)             \n",
    "                if all ([channels[k][p] != channels[k][t] and np.abs(channels[k][p]-channels[k][t]) < 3 and np.any(signals_after_corr[channels[k][p]][times[k][p]]) != 0 and np.any(signals_after_corr[channels[k][t]][times[k][t]]) != 0]):                 \n",
    "                    #print(channels[k][p])\n",
    "                    #print(channels[k][t])\n",
    "                    corr = np.corrcoef(signals_after_corr[channels[k][p]][times[k][p]], signals_after_corr[channels[k][t]][times[k][t]])[0, 1]\n",
    "                    #corr = np.correlate(cut_ch[channels[k][p]][k], cut_ch[channels[k][t]][k])\n",
    "                    #plt.plot(signals_after_corr[channels[k][p]][times[k][p]],'r')\n",
    "                    #plt.plot(signals_after_corr[channels[k][t]][times[k][t]])\n",
    "                    plt.show()\n",
    "#                     print(corr)\n",
    "                    if corr >= th_cor:  #if correlation is more than the threshold\n",
    "#                         print('rowp=', channels[k][p])\n",
    "#                         print('rowt=', channels[k][t])\n",
    "                        if np.abs(X[channels[k][p],rowp[0, k]]) >= maxx[0, k]:\n",
    "                            maxx[0, k] = np.abs(X[channels[k][p],rowp[0, k]])\n",
    "                            #print(maxx[0,k])\n",
    "                        else:    \n",
    "                            signals_after_corr[channels[k][p]][times[k][p]] = np.zeros((1, T1)) #if that signal is not the max for correlation remove it \n",
    "                            zeross = zeross + 1\n",
    "                            #print('rowp', channels[k][p]) #number of channel which is removed\n",
    "                        if np.abs(X[channels[k][t],rowp[0, k]]) > maxx[0, k]: #X[...] is the amount of cut_ch (spike) in peak k\n",
    "                            maxx[0, k] = np.abs(X[channels[k][t],rowp[0, k]])\n",
    "                            signals_after_corr[channels[k][p]][times[k][p]] = np.zeros((1, T1))\n",
    "                            #print('rowp', channels[k][p])\n",
    "                            #print(maxx[0,k])\n",
    "                            zeross = zeross + 1\n",
    "                        else:    \n",
    "                            signals_after_corr[channels[k][t]][times[k][t]] = np.zeros((1, T1))\n",
    "                            zeross = zeross + 1\n",
    "                        #print(maxx[0,k])    \n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating total number of spikes\n",
    "t = 0\n",
    "\n",
    "for i in range (16):\n",
    "    for k in range (np.array(signals_after_corr[i]).shape[0]): #for every peak\n",
    "        if np.any(np.array(signals_after_corr[i])[k]) != 0:\n",
    "            t = t+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors =['dodgerblue', 'skyblue', 'gray','deeppink','orange','saddlebrown','lawngreen', 'g','black','cyan','red','brown','lightgray','yellow','pink','magenta']\n",
    "col = colors[7]\n",
    "\n",
    "#plt.plot(np.array(cluster[3])[:, 0],np.array(cluster[3])[:, 1], col) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(np.array(signals_after_corr).shape[0]):\n",
    "    final_spikes = []\n",
    "    for j in range(np.array(signals_after_corr[i]).shape[0]):\n",
    "        if np.all(signals_after_corr[i][j] == 0): \n",
    "            True_sig = 1  #print(cut_ch[i][j])\n",
    "        else:\n",
    "            final_spikes.append(signals_after_corr[i][j])\n",
    "    if final_spikes != []:        \n",
    "#         print(i + 1) \n",
    "        res = np.array(final_spikes)       \n",
    "        varcovmat = np.cov(res.T)\n",
    "        k = np.array(np.mean(res.T,1))\n",
    "        tt = []\n",
    "        for s in range(res.shape[0]):\n",
    "            tt.append(k)\n",
    "        varcovmat = res.T - np.array(tt).T\n",
    "\n",
    "        u, s, v = svd(varcovmat)\n",
    "        pca_data = np.dot(res,u[:,0:2])\n",
    "        pca_data.shape\n",
    "\n",
    "#         plt.scatter(pca_data[:,0],pca_data[:,1],c=colors[i])\n",
    "\n",
    "#         #plt.xlim([-400,400])\n",
    "#         #plt.ylim([-300,300])\n",
    "#         plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Getting PCA of the data\n",
    "\n",
    "num = t #this is calculated in the previous cell, giving the number of non-zero signals    \n",
    "res = np.zeros((num, T1)) #put non-zero elements in res as arrays\n",
    "chnum = np.zeros((num, 1)) #contains the channel number label for colors in plotting \n",
    "\n",
    "#for determining the labels and groups of clusters\n",
    "\n",
    "clus_spike = []\n",
    "for i in range(16):\n",
    "    clus_spike.append([])\n",
    "\n",
    "p = 0\n",
    "for i in range (16):\n",
    "    for j in range (np.array(signals_after_corr[i]).shape[0]):\n",
    "        if (np.any(np.array(signals_after_corr[i])[j]) != 0): \n",
    "#             while p < np.array(signals_after_corr[i])[j].shape[0]:\n",
    "            res[p,:] = np.array(signals_after_corr[i])[j]\n",
    "            chnum[p,0] = i\n",
    "            data_ch = [i] \n",
    "            p = p + 1\n",
    "            clus_spike[i].append(np.array(signals_after_corr[i])[j])\n",
    "                                  \n",
    "#np.save('clus_spike', clus_spike)            \n",
    "\n",
    "varcovmat = np.cov(res.T)\n",
    "k = np.array(np.mean(res.T,1))\n",
    "tt = []\n",
    "for i in range(res.shape[0]):\n",
    "   tt.append(k)\n",
    "varcovmat = res.T - np.array(tt).T\n",
    "\n",
    "u, s, v = svd(varcovmat)\n",
    "pca_data = np.dot(res,u[:,0:2])\n",
    "pca_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_cluster = 50 # threshold for the number of spikes to be considered as a cluster\n",
    "\n",
    "ch_num = 16\n",
    "all_sorted_size_after=[]\n",
    "for s in range(ch_num):\n",
    "#     print('s=',s)\n",
    "    ch_pca = []\n",
    "    for i in range(pca_data.shape[0]): \n",
    "        #print(int(chnum[i,0]))\n",
    "        if s == int(chnum[i,0]):\n",
    "            ch_pca.append(list(pca_data[i,:]))\n",
    "    ch_pca = np.array(ch_pca)  \n",
    "    if ch_pca.shape[0] > T_cluster:\n",
    "        prenumnode = 7 \n",
    "        GEMsort_dataset = ch_pca\n",
    "        all_sorted_size_after.append(ch_pca.shape[0])\n",
    "        Nodes,GEMsort_C,GEMsort_w, nodes_total,prenumnode,d,nodess = GEMsort(prenumnode, GEMsort_dataset)\n",
    "        \n",
    "        for i in range(GEMsort_C.shape[0]):\n",
    "            for j in range (GEMsort_C.shape[0]):\n",
    "                if GEMsort_C[i,j] != -1:\n",
    "                   # print i,j\n",
    "                    x = [GEMsort_w[i,0],GEMsort_w[j,0]]\n",
    "                    y = [GEMsort_w[i, 1], GEMsort_w[j, 1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sorted_size_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculating F1 score and other parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original clusters information\n",
    "original_cells_spikes_count = np.array([323, 155, 282, 261, 254, 457, 269, 279]) # this comes from the making spikes file and it name is as size_clusters_original\n",
    "all_sorted_size_after = np.array(all_sorted_size_after)\n",
    "\n",
    "# check their sizes\n",
    "len1 = original_cells_spikes_count.shape[0]\n",
    "len2 = all_sorted_size_after.shape[0]\n",
    "for i in range(len1-len2):\n",
    "    if len1 > len2:\n",
    "        all_sorted_size_after = np.append(all_sorted_size_after, 0)\n",
    "    elif len2 > len1:\n",
    "        original_cells_spikes_count = np.append(original_cells_spikes_count, 0)  \n",
    "tp_i = []\n",
    "for i in range(original_cells_spikes_count.shape[0]): \n",
    "    if original_cells_spikes_count[i]> all_sorted_size_after[i]:\n",
    "        tp_i.append(all_sorted_size_after[i])\n",
    "    else:    \n",
    "        tp_i.append(original_cells_spikes_count[i])\n",
    "tp = np.sum(tp_i)\n",
    "fp_i = [] \n",
    "for i in range(original_cells_spikes_count.shape[0]):   \n",
    "    if original_cells_spikes_count[i] < all_sorted_size_after[i]: # kamtar iane eshtebah classify karde\n",
    "        fp_i.append(all_sorted_size_after[i]-original_cells_spikes_count[i])\n",
    "fp = np.sum(fp_i)\n",
    "fn_i = [] \n",
    "for i in range(original_cells_spikes_count.shape[0]):   \n",
    "    if original_cells_spikes_count[i] > all_sorted_size_after[i]: # bishtar iane classify nakarde un meghdaro\n",
    "        fn_i.append(original_cells_spikes_count[i]-all_sorted_size_after[i])\n",
    "fn = np.sum(fn_i)\n",
    "acc = tp/np.sum(tp + fp + fn)\n",
    "pre = tp/np.sum(tp + fp)\n",
    "rec= tp/np.sum(tp + fn)\n",
    "F1 = (2*tp)/np.sum((2*tp) + fp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/.../New figures/Adaptive_threshold/\"\n",
    "# np.save(path + 'F1_normal_data', F1)  # for normal data\n",
    "# np.save(path + 'F1_same_cell', F1) # for the same cell data\n",
    "# np.save(path + 'F1_same_time', F1) # for the same time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data to make a table\n",
    "F1_normal_data = np.load(path + 'F1_normal_data.npy')\n",
    "F1_same_cell = np.load(path + 'F1_same_cell.npy')\n",
    "F1_same_time = np.load(path + 'F1_same_time.npy')\n",
    "print('F1_normal_data=', F1_normal_data)\n",
    "print('F1_same_cell=', F1_same_cell)\n",
    "print('F1_same_time=', F1_same_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "%matplotlib inline \n",
    "data = {\n",
    "    'Performance Metric': ['F1 for Normal Data', 'F1 for Same-cell Data', 'F1 for Same-time Data'],\n",
    "    'Values': [np.round(F1_normal_data.item(), 3), \\\n",
    "               np.round(F1_same_cell.item(), 3),\\\n",
    "               np.round(F1_same_time.item(), 3)]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a figure and a single subplot\n",
    "fig, ax = plt.subplots(figsize=(5, 2))  # Adjust the size as needed\n",
    "\n",
    "# Hide axes\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=df.values, colLabels=df.columns, loc='center', cellLoc='center')\n",
    "\n",
    "# Adjust table style\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2)\n",
    "\n",
    "# Add a title\n",
    "fig.suptitle('F1 Scores for Sorting with Dynamic Thresholding', fontsize=10, y=.95)\n",
    "\n",
    "# Save the table as a PDF\n",
    "# plt.savefig(path + 'F1_adaptive_threshold_table.pdf')\n",
    "\n",
    "# Show the table\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
