{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b32c46fb",
   "metadata": {},
   "source": [
    "### Overlapping Segmentation for Improved Spike Detection in Neural Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf63036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6508f1",
   "metadata": {},
   "source": [
    "### A Performance Test: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76927e89",
   "metadata": {},
   "source": [
    "A swift evaluation to examine how incorporating overlap affects performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc882015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "path = '/.../New figures/' + 'Overlap_segmentation/'\n",
    "\n",
    "# Define data parameters\n",
    "data_length = 10000\n",
    "num_channels = 16\n",
    "\n",
    "# Create synthetic spike data with noise\n",
    "spike_data = np.random.randn(data_length, num_channels)\n",
    "spike_amplitude = 5  \n",
    "spike_duration = 50 \n",
    "for i in range(200):  \n",
    "    spike_time = np.random.randint(0, data_length - spike_duration)\n",
    "    spike_channels = np.random.choice(num_channels, size=3, replace=False)  \n",
    "    spike_profile = np.sin(np.linspace(0, np.pi, spike_duration)) * spike_amplitude\n",
    "    for channel in spike_channels:\n",
    "        spike_data[spike_time:(spike_time + spike_duration), channel] += spike_profile\n",
    "\n",
    "# Function to segment data based on number of channels with optional overlap\n",
    "def segment_data_by_channels(data, segment_channel_size, overlap_channels):\n",
    "    segments = []\n",
    "    start_channel = 0\n",
    "    while start_channel + segment_channel_size <= num_channels:\n",
    "        end_channel = start_channel + segment_channel_size + overlap_channels\n",
    "        end_channel = end_channel if end_channel <= num_channels else num_channels\n",
    "        segments.append(data[:, start_channel:end_channel])\n",
    "        start_channel += segment_channel_size\n",
    "    return segments\n",
    "\n",
    "def detect_spikes(segments, threshold=3.5): \n",
    "    spike_counts = 0\n",
    "    debounce_time = 60\n",
    "    for segment in segments:\n",
    "        for i in range(debounce_time, segment.shape[0] - debounce_time, debounce_time):\n",
    "            max_amplitude = np.max(np.abs(segment[i-debounce_time:i+debounce_time]))\n",
    "            if max_amplitude > threshold:\n",
    "                spike_counts += 1\n",
    "    return spike_counts\n",
    "\n",
    "\n",
    "segment_channel_size = 4\n",
    "\n",
    "# Define high overlap \n",
    "overlap_channels = 2  \n",
    "\n",
    "segments_without_overlap = segment_data_by_channels(spike_data, segment_channel_size, 0)\n",
    "segments_with_overlap = segment_data_by_channels(spike_data, segment_channel_size, overlap_channels)\n",
    "spikes_detected_without_overlap = detect_spikes(segments_without_overlap)\n",
    "spikes_detected_with_overlap = detect_spikes(segments_with_overlap)\n",
    "\n",
    "if spikes_detected_without_overlap > 0:\n",
    "    proportional_increase = ((spikes_detected_with_overlap - spikes_detected_without_overlap) / spikes_detected_without_overlap) * 100\n",
    "else:\n",
    "    proportional_increase = \"undefined\"  \n",
    "\n",
    "# Print results\n",
    "print(f\"Spikes detected without overlap: {spikes_detected_without_overlap}\")\n",
    "print(f\"Spikes detected with overlap: {spikes_detected_with_overlap}\")\n",
    "print(f\"Proportional increase in spike detection with overlap: {proportional_increase}%\")\n",
    "\n",
    "# Define low overlap \n",
    "overlap_channels = 1  \n",
    "\n",
    "segments_without_overlap = segment_data_by_channels(spike_data, segment_channel_size, 0)\n",
    "segments_with_overlap = segment_data_by_channels(spike_data, segment_channel_size, overlap_channels)\n",
    "spikes_detected_without_overlap = detect_spikes(segments_without_overlap)\n",
    "spikes_detected_with_overlap = detect_spikes(segments_with_overlap)\n",
    "\n",
    "if spikes_detected_without_overlap > 0:\n",
    "    proportional_increase = ((spikes_detected_with_overlap - spikes_detected_without_overlap) / spikes_detected_without_overlap) * 100\n",
    "else:\n",
    "    proportional_increase = \"undefined\"  \n",
    "\n",
    "# Print results\n",
    "print(f\"Spikes detected without overlap: {spikes_detected_without_overlap}\")\n",
    "print(f\"Spikes detected with overlap: {spikes_detected_with_overlap}\")\n",
    "print(f\"Proportional increase in spike detection with overlap: {proportional_increase}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d96f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create DataFrame\n",
    "data = {\n",
    "    'Approach': [\"High Overlap\", \"Low Overlap\"],\n",
    "    'Spikes Detected w/o Considering Overlap': [582, 582],\n",
    "    'Spikes Detected w/ Considering Overlap': [614, 599],\n",
    "    'Increase in Detection (%)': [5.49, 2.92]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a PDF file\n",
    "with PdfPages(path + \"/overlap_performance.pdf\") as pdf:\n",
    "    fig, ax = plt.subplots(figsize=(7, 3))  # Adjust the figsize as needed\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    col_widths = [0.12, 0.27, 0.27, 0.2]  # Adjust widths to fit your needs\n",
    "\n",
    "    # Create the table\n",
    "    table = ax.table(cellText=df.values, colLabels=df.columns, loc='center', \n",
    "                     cellLoc='center', colWidths=col_widths)  # cellLoc for text alignment\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(6)  # Set the font size for the table\n",
    "    table.scale(1, 1.5)  # Adjust table scale\n",
    "    ax.text(0.5, .75, 'Performance Summary: Detection Overlap', transform=ax.transAxes,\n",
    "        fontsize=9, fontweight='bold', va='top', ha='center')\n",
    "\n",
    "    # Adjust layout to make sure the table is within the figure area\n",
    "    plt.tight_layout()\n",
    "\n",
    "#     pdf.savefig(fig, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaec48e",
   "metadata": {},
   "source": [
    "### Finding best segment size: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba468989",
   "metadata": {},
   "source": [
    "Acknowledging that the effectiveness of overlap depends on data characteristics, we incorporated an optimization component into our algorithm and conducted further simulations to refine our segmentation approach. We aimed to identify the best segment size that effectively captures overlaps while minimizing detection redundancy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd9df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_channels = 64\n",
    "num_time_points = 1000\n",
    "correlated_block_start = 29  \n",
    "correlated_block_size = 6\n",
    "correlation_probability_high = 0.2  \n",
    "spike_duration = 20  \n",
    "jitter_high = 2  \n",
    "\n",
    "# Initialize spike matrix\n",
    "spike_matrix = np.zeros((num_channels, num_time_points))\n",
    "\n",
    "def gaussian_waveform(center, length=20, height=1):\n",
    "    x = np.arange(0, length)\n",
    "    sigma = length / 6 \n",
    "    return height * np.exp(-np.power(x - center, 2.) / (2 * np.power(sigma, 2.)))\n",
    "waveform_center = spike_duration // 2\n",
    "\n",
    "np.random.seed(42)  # For reproducibility\n",
    "for t in range(num_time_points):\n",
    "    if np.random.rand() < correlation_probability_high:\n",
    "        start_time = t + np.random.randint(-jitter_high, jitter_high + 1)\n",
    "        waveform = gaussian_waveform(waveform_center, spike_duration)\n",
    "        for channel in range(correlated_block_start, correlated_block_start + correlated_block_size):\n",
    "            adjusted_start = max(0, start_time + np.random.randint(-jitter_high, jitter_high + 1))\n",
    "            adjusted_end = min(num_time_points, adjusted_start + spike_duration)\n",
    "            if adjusted_end > adjusted_start:  \n",
    "                adjusted_waveform = waveform[:adjusted_end - adjusted_start]\n",
    "                spike_matrix[channel, adjusted_start:adjusted_end] += adjusted_waveform\n",
    "\n",
    "def count_strict_correlated_spikes(data, max_segment_size, threshold=0.5):\n",
    "    num_channels, num_time_points = data.shape\n",
    "    max_correlated_counts = []\n",
    "    for segment_size in range(2, max_segment_size + 1):\n",
    "        max_count = 0\n",
    "        for start_channel in range(num_channels - segment_size + 1):\n",
    "            segment = data[start_channel:start_channel + segment_size, :]\n",
    "            correlated_spikes = (np.sum(segment, axis=0) >= threshold * segment_size).sum()\n",
    "            max_count = max(max_count, correlated_spikes)\n",
    "        max_correlated_counts.append(max_count)\n",
    "    return max_correlated_counts\n",
    "\n",
    "strict_correlated_spike_counts = count_strict_correlated_spikes(spike_matrix, num_channels)\n",
    "plt.figure(figsize=(7, 3))\n",
    "T= int(num_channels/2) + 1\n",
    "plt.plot(range(2, T), strict_correlated_spike_counts[0:T-2], marker='o', markersize=5)\n",
    "plt.xlabel('Segment Size (Number of Channels)')\n",
    "plt.ylabel('Max Correlated Spike Count')\n",
    "\n",
    "ax = plt.gca()\n",
    "highlight_x = 6\n",
    "highlight_y = strict_correlated_spike_counts[highlight_x-2]\n",
    "\n",
    "ax.text(highlight_x, highlight_y, ' ', color='red', size=4, \n",
    "        bbox=dict(facecolor='none', edgecolor='green'))\n",
    "\n",
    "plt.title('Correlated Spike Count: Single Segment Size')\n",
    "plt.grid(True)\n",
    "plt.ylim(100, 980)\n",
    "# plt.savefig(path +'Correlated Spike Count: Single Segment Size.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f8aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "num_channels = 64\n",
    "num_time_points = 1000\n",
    "neuron_blocks = [(10, 4), (25, 6), (40, 8), (55, 9)]\n",
    "correlation_probability_high = 0.5  \n",
    "spike_duration = 20\n",
    "jitter_high = 1  \n",
    "\n",
    "# Initialize spike matrix\n",
    "spike_matrix = np.zeros((num_channels, num_time_points))\n",
    "def gaussian_waveform(center, length=20, height=1):\n",
    "    x = np.arange(0, length)\n",
    "    sigma = length / 6\n",
    "    return height * np.exp(-np.power(x - center, 2.) / (2 * np.power(sigma, 2.)))\n",
    "\n",
    "np.random.seed(42)\n",
    "for block_start, block_size in neuron_blocks:\n",
    "    for t in range(num_time_points):\n",
    "        if np.random.rand() < correlation_probability_high:\n",
    "            start_time = t + np.random.randint(-jitter_high, jitter_high + 1)\n",
    "            waveform = gaussian_waveform(spike_duration // 2, spike_duration)\n",
    "            for channel in range(block_start, block_start + block_size):\n",
    "                adjusted_start = max(0, start_time)  \n",
    "                adjusted_end = min(num_time_points, adjusted_start + spike_duration)  \n",
    "                if adjusted_end > adjusted_start:  \n",
    "                    adjusted_waveform = waveform[:adjusted_end - adjusted_start]  \n",
    "                    spike_matrix[channel, adjusted_start:adjusted_end] += adjusted_waveform\n",
    "\n",
    "def count_correlated_spikes(data, max_segment_size):\n",
    "    num_channels, num_time_points = data.shape\n",
    "    max_correlated_counts = []\n",
    "    for segment_size in range(2, max_segment_size + 1):\n",
    "        max_count = 0\n",
    "        for start_channel in range(num_channels - segment_size + 1):\n",
    "            segment = data[start_channel:start_channel + segment_size, :]\n",
    "            correlated_spikes = (np.sum(segment, axis=0) >= 0.85 * segment_size).sum()\n",
    "            max_count = max(max_count, correlated_spikes)\n",
    "        max_correlated_counts.append(max_count)\n",
    "    return max_correlated_counts\n",
    "\n",
    "correlated_spike_counts = count_correlated_spikes(spike_matrix, num_channels)\n",
    "\n",
    "plt.figure(figsize=(7, 3))\n",
    "T= int(num_channels/2) + 1\n",
    "plt.plot(range(2, T), correlated_spike_counts[0:T-2], marker='o', markersize=5)\n",
    "# plt.xlim(0, 32.2)\n",
    "ax = plt.gca()\n",
    "highlight_x = 9\n",
    "highlight_y = correlated_spike_counts[highlight_x-2]\n",
    "\n",
    "ax.text(highlight_x, highlight_y, ' ', color='red', size=4, \n",
    "        bbox=dict(facecolor='none', edgecolor='green'))\n",
    "plt.xlabel('Segment Size (Number of Channels)')\n",
    "plt.ylabel('Max Correlated Spike Count')\n",
    "plt.title('Correlated Spike Count: Various Segment Sizes')\n",
    "plt.grid(True)\n",
    "plt.ylim(986, 995.5)\n",
    "# plt.savefig(path +'Correlated Spike Count: Various Segment Size.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f757df76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
