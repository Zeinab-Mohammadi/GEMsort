{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aaa39b2",
   "metadata": {},
   "source": [
    "### Determining the Optimum Number of Channels with Correlated Neural Signals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaa6b42",
   "metadata": {},
   "source": [
    "This Jupyter notebook contains designed to dynamically assess and optimize the number of recording sites within a probe's columns based on significant correlations in neural recording data. The approach adjusts for variability in experimental conditions, dataset properties, brain areas, and probe designs. By calculating the average correlation among recording sites and comparing it against a predefined threshold, the script determines the optimal number of channels that exhibit similar neural activity patterns. This method provides a flexible and accurate way to tailor probe configuration to various experimental setups, enhancing the reliability of neural data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba1a133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18566cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining related functions\n",
    "\n",
    "def smooth_neural_data(signal, window_size=10):\n",
    "    \"\"\"Apply a moving average to make the neural activity pattern smoother.\"\"\"\n",
    "    return np.convolve(signal, np.ones(window_size)/window_size, mode='same')\n",
    "\n",
    "# Function to calculate average correlation for each recording site within a column\n",
    "def average_correlation(data, num_sites, threshold):\n",
    "    avg_correlations = []\n",
    "    for i in range(num_sites):\n",
    "        correlations = []\n",
    "        for j in range(num_sites):\n",
    "            if i != j:\n",
    "                corr, _ = pearsonr(data[i], data[j])\n",
    "                correlations.append(corr)\n",
    "        avg_corr = np.mean(correlations) if correlations else 0\n",
    "        avg_correlations.append(avg_corr)\n",
    "    significant_sites = sum(corr > threshold for corr in avg_correlations)\n",
    "    return significant_sites\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc3a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the initial window of neural data\n",
    "np.random.seed(42)\n",
    "num_columns = 3\n",
    "max_sites_per_column = 50\n",
    "data_length = 1000  \n",
    "correlation_threshold = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dad6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_neural_activity(rate, length, shared_activity=None, amplitude=1):\n",
    "    \"\"\"Generates a random neural activity pattern for a neuron, including shared activity.\"\"\"\n",
    "    times = np.random.rand(int(rate * length)) * length  # Random neural activity times\n",
    "    neural_activity = np.zeros(length)\n",
    "    neural_activity[np.floor(times).astype(int)] = amplitude  # Set activity\n",
    "    \n",
    "    if shared_activity is not None:\n",
    "        neural_activity += shared_activity  # Add shared activity to simulate more correlation\n",
    "\n",
    "    return neural_activity\n",
    "\n",
    "\n",
    "initial_neural_data = []\n",
    "for col in range(num_columns):\n",
    "    shared_activity = generate_neural_activity(0.05, data_length, amplitude=5)  # Shared component\n",
    "    column_data = [smooth_neural_data(generate_neural_activity(0.05, data_length, shared_activity, np.random.rand()*5))\n",
    "                   for _ in range(max_sites_per_column)]\n",
    "    initial_neural_data.extend(column_data)\n",
    "\n",
    "initial_neural_data = np.array(initial_neural_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cdccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each column for the initial data window\n",
    "optimal_k = []\n",
    "for col in range(num_columns):\n",
    "    col_data = initial_neural_data[col * max_sites_per_column:(col + 1) * max_sites_per_column]\n",
    "    significant_sites = average_correlation(col_data, max_sites_per_column, correlation_threshold)\n",
    "    optimal_k.append(significant_sites)\n",
    "\n",
    "print(\"Number of recording sites per column with correlations > threshold:\", optimal_k)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
